{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPB06IyXALSMwQ+4DfAMINO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgevatschnaider/NLP_Projecto/blob/main/NLP_Tokenizaci%C3%B3n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gtb3i3Nm9lh0",
        "outputId": "d295e122-9793-4c0b-bd94-d44b47b2d09b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<!DOCTYPE html>\n",
              "<html lang=\"es\">\n",
              "<head>\n",
              "    <meta charset=\"UTF-8\">\n",
              "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
              "    <title>Tokenización en NLP</title>\n",
              "    <link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap\" rel=\"stylesheet\">\n",
              "    <style>\n",
              "        body {\n",
              "            font-family: 'Roboto', Arial, sans-serif;\n",
              "            line-height: 1.6;\n",
              "            margin: 20px;\n",
              "            padding: 20px;\n",
              "            background-color: #f9f9f9;\n",
              "            color: #333;\n",
              "        }\n",
              "        h1 {\n",
              "            color: #2c3e50;\n",
              "            text-align: center;\n",
              "            font-size: 2.5em;\n",
              "            margin-bottom: 20px;\n",
              "        }\n",
              "        h2 {\n",
              "            color: #2980b9;\n",
              "            font-size: 1.8em;\n",
              "            margin-top: 30px;\n",
              "        }\n",
              "        p {\n",
              "            font-size: 1.2em;\n",
              "            margin-bottom: 15px;\n",
              "        }\n",
              "        code {\n",
              "            background-color: #f0f0f0;\n",
              "            padding: 2px 5px;\n",
              "            border-radius: 3px;\n",
              "            font-family: 'Courier New', Courier, monospace;\n",
              "        }\n",
              "        pre {\n",
              "            background-color: #f0f0f0;\n",
              "            padding: 10px;\n",
              "            border-radius: 5px;\n",
              "            font-size: 1.1em;\n",
              "            overflow-x: auto;\n",
              "        }\n",
              "    </style>\n",
              "</head>\n",
              "<body>\n",
              "    <h1>Tokenización: Un Paso Clave en el Procesamiento del Lenguaje Natural</h1>\n",
              "    \n",
              "    <p>La tokenización es una de las primeras y más importantes etapas en el procesamiento del lenguaje natural (NLP). Consiste en dividir un texto grande y continuo en unidades más pequeñas llamadas <strong>tokens</strong>. Estas unidades permiten que el texto sea más manejable y que los algoritmos de NLP puedan trabajar con mayor facilidad. Los tokens pueden ser palabras, subpalabras, caracteres o incluso oraciones, dependiendo del método de tokenización que se aplique.</p>\n",
              "\n",
              "    <h2>¿Por qué es importante la tokenización?</h2>\n",
              "    <p>El texto sin procesar es una secuencia de caracteres sin estructura definida desde el punto de vista de una máquina. Los modelos y algoritmos de NLP requieren que el texto esté en una forma estructurada y entendible. Aquí es donde la tokenización juega un papel clave, transformando la secuencia no estructurada en componentes básicos que puedan ser analizados y utilizados por sistemas automatizados.</p>\n",
              "\n",
              "    <h2>Métodos comunes de tokenización</h2>\n",
              "    <ul>\n",
              "        <li><strong>Tokenización a nivel de palabras</strong>: El texto se divide en palabras individuales. Cada palabra se considera un token. Es útil para tareas como análisis de sentimientos o clasificación de textos.</li>\n",
              "    </ul>\n",
              "\n",
              "    <pre><code>['El', 'sol', 'brilla']</code></pre>\n",
              "\n",
              "    <ul>\n",
              "        <li><strong>Tokenización a nivel de subpalabras</strong>: Se dividen las palabras en fragmentos más pequeños llamados subpalabras. Útil para manejar palabras raras o desconocidas, utilizado por modelos como BERT o GPT.</li>\n",
              "    </ul>\n",
              "\n",
              "    <pre><code>['Token', 'ization']</code></pre>\n",
              "\n",
              "    <ul>\n",
              "        <li><strong>Tokenización a nivel de caracteres</strong>: Cada carácter individual es tratado como un token. Útil para lenguajes sin delimitadores claros, como el chino o japonés.</li>\n",
              "    </ul>\n",
              "\n",
              "    <pre><code>['H', 'o', 'l', 'a']</code></pre>\n",
              "\n",
              "    <ul>\n",
              "        <li><strong>Tokenización basada en oraciones</strong>: El texto se divide en oraciones completas, en lugar de palabras o caracteres. Ideal para tareas como análisis sintáctico o resumen de textos.</li>\n",
              "    </ul>\n",
              "\n",
              "    <pre><code>['Hola.', '¿Cómo estás?']</code></pre>\n",
              "\n",
              "    <h2>Métodos de tokenización empleados en la práctica</h2>\n",
              "    <p>En Python, bibliotecas como <code>spaCy</code>, <code>NLTK</code> o <code>Hugging Face's Tokenizers</code> proveen métodos eficientes para tokenización.</p>\n",
              "    \n",
              "    <p>Ejemplo sencillo de tokenización con NLTK:</p>\n",
              "\n",
              "    <pre><code>import nltk\n",
              "nltk.download('punkt')\n",
              "from nltk.tokenize import word_tokenize\n",
              "\n",
              "text = \"Tokenizing text is a core task of NLP.\"\n",
              "tokens = word_tokenize(text)\n",
              "print(tokens)</code></pre>\n",
              "\n",
              "    <p>Salida:</p>\n",
              "\n",
              "    <pre><code>['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP', '.']</code></pre>\n",
              "\n",
              "    <h2>Consideraciones finales</h2>\n",
              "    <p>La tokenización es esencial en todas las aplicaciones de NLP porque convierte el lenguaje humano en algo que las máquinas pueden entender. El método de tokenización que elijas dependerá de la tarea específica y de los requerimientos del modelo. Elegir entre tokenización a nivel de palabra, subpalabra o carácter puede afectar la precisión y eficiencia del modelo.</p>\n",
              "</body>\n",
              "</html>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "# Contenido HTML sobre Tokenización\n",
        "html_content = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"es\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Tokenización en NLP</title>\n",
        "    <link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap\" rel=\"stylesheet\">\n",
        "    <style>\n",
        "        body {\n",
        "            font-family: 'Roboto', Arial, sans-serif;\n",
        "            line-height: 1.6;\n",
        "            margin: 20px;\n",
        "            padding: 20px;\n",
        "            background-color: #f9f9f9;\n",
        "            color: #333;\n",
        "        }\n",
        "        h1 {\n",
        "            color: #2c3e50;\n",
        "            text-align: center;\n",
        "            font-size: 2.5em;\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "        h2 {\n",
        "            color: #2980b9;\n",
        "            font-size: 1.8em;\n",
        "            margin-top: 30px;\n",
        "        }\n",
        "        p {\n",
        "            font-size: 1.2em;\n",
        "            margin-bottom: 15px;\n",
        "        }\n",
        "        code {\n",
        "            background-color: #f0f0f0;\n",
        "            padding: 2px 5px;\n",
        "            border-radius: 3px;\n",
        "            font-family: 'Courier New', Courier, monospace;\n",
        "        }\n",
        "        pre {\n",
        "            background-color: #f0f0f0;\n",
        "            padding: 10px;\n",
        "            border-radius: 5px;\n",
        "            font-size: 1.1em;\n",
        "            overflow-x: auto;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Tokenización: Un Paso Clave en el Procesamiento del Lenguaje Natural</h1>\n",
        "\n",
        "    <p>La tokenización es una de las primeras y más importantes etapas en el procesamiento del lenguaje natural (NLP). Consiste en dividir un texto grande y continuo en unidades más pequeñas llamadas <strong>tokens</strong>. Estas unidades permiten que el texto sea más manejable y que los algoritmos de NLP puedan trabajar con mayor facilidad. Los tokens pueden ser palabras, subpalabras, caracteres o incluso oraciones, dependiendo del método de tokenización que se aplique.</p>\n",
        "\n",
        "    <h2>¿Por qué es importante la tokenización?</h2>\n",
        "    <p>El texto sin procesar es una secuencia de caracteres sin estructura definida desde el punto de vista de una máquina. Los modelos y algoritmos de NLP requieren que el texto esté en una forma estructurada y entendible. Aquí es donde la tokenización juega un papel clave, transformando la secuencia no estructurada en componentes básicos que puedan ser analizados y utilizados por sistemas automatizados.</p>\n",
        "\n",
        "    <h2>Métodos comunes de tokenización</h2>\n",
        "    <ul>\n",
        "        <li><strong>Tokenización a nivel de palabras</strong>: El texto se divide en palabras individuales. Cada palabra se considera un token. Es útil para tareas como análisis de sentimientos o clasificación de textos.</li>\n",
        "    </ul>\n",
        "\n",
        "    <pre><code>['El', 'sol', 'brilla']</code></pre>\n",
        "\n",
        "    <ul>\n",
        "        <li><strong>Tokenización a nivel de subpalabras</strong>: Se dividen las palabras en fragmentos más pequeños llamados subpalabras. Útil para manejar palabras raras o desconocidas, utilizado por modelos como BERT o GPT.</li>\n",
        "    </ul>\n",
        "\n",
        "    <pre><code>['Token', 'ization']</code></pre>\n",
        "\n",
        "    <ul>\n",
        "        <li><strong>Tokenización a nivel de caracteres</strong>: Cada carácter individual es tratado como un token. Útil para lenguajes sin delimitadores claros, como el chino o japonés.</li>\n",
        "    </ul>\n",
        "\n",
        "    <pre><code>['H', 'o', 'l', 'a']</code></pre>\n",
        "\n",
        "    <ul>\n",
        "        <li><strong>Tokenización basada en oraciones</strong>: El texto se divide en oraciones completas, en lugar de palabras o caracteres. Ideal para tareas como análisis sintáctico o resumen de textos.</li>\n",
        "    </ul>\n",
        "\n",
        "    <pre><code>['Hola.', '¿Cómo estás?']</code></pre>\n",
        "\n",
        "    <h2>Métodos de tokenización empleados en la práctica</h2>\n",
        "    <p>En Python, bibliotecas como <code>spaCy</code>, <code>NLTK</code> o <code>Hugging Face's Tokenizers</code> proveen métodos eficientes para tokenización.</p>\n",
        "\n",
        "    <p>Ejemplo sencillo de tokenización con NLTK:</p>\n",
        "\n",
        "    <pre><code>import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Tokenizing text is a core task of NLP.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)</code></pre>\n",
        "\n",
        "    <p>Salida:</p>\n",
        "\n",
        "    <pre><code>['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP', '.']</code></pre>\n",
        "\n",
        "    <h2>Consideraciones finales</h2>\n",
        "    <p>La tokenización es esencial en todas las aplicaciones de NLP porque convierte el lenguaje humano en algo que las máquinas pueden entender. El método de tokenización que elijas dependerá de la tarea específica y de los requerimientos del modelo. Elegir entre tokenización a nivel de palabra, subpalabra o carácter puede afectar la precisión y eficiencia del modelo.</p>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Mostrar el contenido HTML en Google Colab\n",
        "display(HTML(html_content))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Descargamos los recursos necesarios para tokenización, etiquetado y stemming\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Importamos las funcionalidades necesarias\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Frase para analizar en inglés\n",
        "text = \"Artificial intelligence is going to revolutionize the world of work.\"\n",
        "\n",
        "# Tokenización: dividir el texto en palabras\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Etiquetado gramatical (POS tagging): obtener la categoría gramatical de cada palabra\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "print(\"Etiquetas gramaticales:\", tagged_tokens)\n",
        "\n",
        "# Stemming: reducir las palabras a su raíz\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "print(\"Stemming:\", stemmed_tokens)\n",
        "\n",
        "# Lematización: reducir las palabras a su forma base\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(\"Lematización:\", lemmatized_tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V134bTT9AT6i",
        "outputId": "b9522a3f-7ed5-4899-f1d8-0c1454d69058"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Artificial', 'intelligence', 'is', 'going', 'to', 'revolutionize', 'the', 'world', 'of', 'work', '.']\n",
            "Etiquetas gramaticales: [('Artificial', 'JJ'), ('intelligence', 'NN'), ('is', 'VBZ'), ('going', 'VBG'), ('to', 'TO'), ('revolutionize', 'VB'), ('the', 'DT'), ('world', 'NN'), ('of', 'IN'), ('work', 'NN'), ('.', '.')]\n",
            "Stemming: ['artifici', 'intellig', 'is', 'go', 'to', 'revolution', 'the', 'world', 'of', 'work', '.']\n",
            "Lematización: ['Artificial', 'intelligence', 'is', 'going', 'to', 'revolutionize', 'the', 'world', 'of', 'work', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "# Contenido HTML sobre Introducción a NLTK\n",
        "html_content = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"es\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Introducción a NLTK</title>\n",
        "    <link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap\" rel=\"stylesheet\">\n",
        "    <style>\n",
        "        body {\n",
        "            font-family: 'Roboto', Arial, sans-serif;\n",
        "            line-height: 1.6;\n",
        "            margin: 20px;\n",
        "            padding: 20px;\n",
        "            background-color: #f9f9f9;\n",
        "            color: #333;\n",
        "        }\n",
        "        h1 {\n",
        "            color: #2c3e50;\n",
        "            text-align: center;\n",
        "            font-size: 2.5em;\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "        h2 {\n",
        "            color: #2980b9;\n",
        "            font-size: 1.8em;\n",
        "            margin-top: 30px;\n",
        "        }\n",
        "        p {\n",
        "            font-size: 1.2em;\n",
        "            margin-bottom: 15px;\n",
        "        }\n",
        "        code {\n",
        "            background-color: #f0f0f0;\n",
        "            padding: 2px 5px;\n",
        "            border-radius: 3px;\n",
        "            font-family: 'Courier New', Courier, monospace;\n",
        "        }\n",
        "        pre {\n",
        "            background-color: #f0f0f0;\n",
        "            padding: 10px;\n",
        "            border-radius: 5px;\n",
        "            font-size: 1.1em;\n",
        "            overflow-x: auto;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Introducción a NLTK: El Kit de Herramientas para el Procesamiento del Lenguaje Natural</h1>\n",
        "\n",
        "    <p><strong>NLTK</strong> (Natural Language Toolkit) es una biblioteca de Python ampliamente utilizada para el procesamiento del lenguaje natural (NLP). Proporciona una amplia gama de herramientas que permiten trabajar con texto de manera efectiva, como:</p>\n",
        "\n",
        "    <ul>\n",
        "        <li>Tokenización: Dividir un texto en palabras o frases.</li>\n",
        "        <li>Etiquetado gramatical (POS tagging): Asignar categorías gramaticales a las palabras de un texto.</li>\n",
        "        <li>Stemming: Reducir las palabras a su raíz o forma base.</li>\n",
        "        <li>Lematización: Convertir palabras a su forma base o \"diccionario\".</li>\n",
        "        <li>Acceso a corpus: Incluye acceso a varios corpus de texto precompilados.</li>\n",
        "    </ul>\n",
        "\n",
        "    <h2>¿Por qué usar NLTK?</h2>\n",
        "    <p>NLTK es ideal tanto para principiantes como para expertos en NLP. Te permite realizar tareas complejas de procesamiento de texto de manera sencilla utilizando módulos predefinidos.</p>\n",
        "\n",
        "    <h2>Ejemplo de Código con NLTK</h2>\n",
        "    <p>El siguiente código muestra cómo usar algunas de las funcionalidades más básicas de NLTK: tokenización, etiquetado gramatical (POS tagging), stemming, y lematización.</p>\n",
        "\n",
        "    <pre><code>import nltk\n",
        "# Descargar recursos necesarios\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Importar las funcionalidades necesarias\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Frase para analizar\n",
        "text = \"Artificial intelligence is going to revolutionize the world of work.\"\n",
        "\n",
        "# Tokenización: dividir el texto en palabras\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Etiquetado gramatical: obtener la categoría gramatical de cada palabra\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "print(\"Etiquetas gramaticales:\", tagged_tokens)\n",
        "\n",
        "# Stemming: reducir las palabras a su raíz\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "print(\"Stemming:\", stemmed_tokens)\n",
        "\n",
        "# Lematización: reducir las palabras a su forma base\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(\"Lematización:\", lemmatized_tokens)</code></pre>\n",
        "\n",
        "    <h2>Explicación del Código</h2>\n",
        "    <ul>\n",
        "        <li><strong>Tokenización</strong>: Convierte el texto en una lista de palabras o tokens.</li>\n",
        "        <li><strong>Etiquetado gramatical</strong>: Asigna una categoría gramatical a cada palabra.</li>\n",
        "        <li><strong>Stemming</strong>: Reduce las palabras a su raíz o forma básica.</li>\n",
        "        <li><strong>Lematización</strong>: Convierte las palabras a su forma base o diccionario.</li>\n",
        "    </ul>\n",
        "\n",
        "    <h2>Salida esperada</h2>\n",
        "    <pre><code>Tokens: ['Artificial', 'intelligence', 'is', 'going', 'to', 'revolutionize', 'the', 'world', 'of', 'work', '.']\n",
        "Etiquetas gramaticales: [('Artificial', 'JJ'), ('intelligence', 'NN'), ('is', 'VBZ'), ('going', 'VBG'), ('to', 'TO'), ('revolutionize', 'VB'), ('the', 'DT'), ('world', 'NN'), ('of', 'IN'), ('work', 'NN'), ('.', '.')]\n",
        "Stemming: ['artifici', 'intellig', 'is', 'go', 'to', 'revolution', 'the', 'world', 'of', 'work', '.']\n",
        "Lematización: ['Artificial', 'intelligence', 'is', 'going', 'to', 'revolutionize', 'the', 'world', 'of', 'work', '.']</code></pre>\n",
        "\n",
        "    <h2>Conclusión</h2>\n",
        "    <p>NLTK es una poderosa biblioteca para el procesamiento del lenguaje natural. Con este código, has aprendido cómo tokenizar, etiquetar gramaticalmente, aplicar stemming y lematización a un texto. NLTK ofrece una forma rápida y sencilla de abordar tareas complejas de procesamiento de texto.</p>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Mostrar el contenido HTML en Google Colab\n",
        "display(HTML(html_content))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1L3xdlw8Cqmc",
        "outputId": "2e605e7a-085d-4275-d60c-f7006176cf28"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<!DOCTYPE html>\n",
              "<html lang=\"es\">\n",
              "<head>\n",
              "    <meta charset=\"UTF-8\">\n",
              "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
              "    <title>Introducción a NLTK</title>\n",
              "    <link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap\" rel=\"stylesheet\">\n",
              "    <style>\n",
              "        body {\n",
              "            font-family: 'Roboto', Arial, sans-serif;\n",
              "            line-height: 1.6;\n",
              "            margin: 20px;\n",
              "            padding: 20px;\n",
              "            background-color: #f9f9f9;\n",
              "            color: #333;\n",
              "        }\n",
              "        h1 {\n",
              "            color: #2c3e50;\n",
              "            text-align: center;\n",
              "            font-size: 2.5em;\n",
              "            margin-bottom: 20px;\n",
              "        }\n",
              "        h2 {\n",
              "            color: #2980b9;\n",
              "            font-size: 1.8em;\n",
              "            margin-top: 30px;\n",
              "        }\n",
              "        p {\n",
              "            font-size: 1.2em;\n",
              "            margin-bottom: 15px;\n",
              "        }\n",
              "        code {\n",
              "            background-color: #f0f0f0;\n",
              "            padding: 2px 5px;\n",
              "            border-radius: 3px;\n",
              "            font-family: 'Courier New', Courier, monospace;\n",
              "        }\n",
              "        pre {\n",
              "            background-color: #f0f0f0;\n",
              "            padding: 10px;\n",
              "            border-radius: 5px;\n",
              "            font-size: 1.1em;\n",
              "            overflow-x: auto;\n",
              "        }\n",
              "    </style>\n",
              "</head>\n",
              "<body>\n",
              "    <h1>Introducción a NLTK: El Kit de Herramientas para el Procesamiento del Lenguaje Natural</h1>\n",
              "    \n",
              "    <p><strong>NLTK</strong> (Natural Language Toolkit) es una biblioteca de Python ampliamente utilizada para el procesamiento del lenguaje natural (NLP). Proporciona una amplia gama de herramientas que permiten trabajar con texto de manera efectiva, como:</p>\n",
              "\n",
              "    <ul>\n",
              "        <li>Tokenización: Dividir un texto en palabras o frases.</li>\n",
              "        <li>Etiquetado gramatical (POS tagging): Asignar categorías gramaticales a las palabras de un texto.</li>\n",
              "        <li>Stemming: Reducir las palabras a su raíz o forma base.</li>\n",
              "        <li>Lematización: Convertir palabras a su forma base o \"diccionario\".</li>\n",
              "        <li>Acceso a corpus: Incluye acceso a varios corpus de texto precompilados.</li>\n",
              "    </ul>\n",
              "\n",
              "    <h2>¿Por qué usar NLTK?</h2>\n",
              "    <p>NLTK es ideal tanto para principiantes como para expertos en NLP. Te permite realizar tareas complejas de procesamiento de texto de manera sencilla utilizando módulos predefinidos.</p>\n",
              "\n",
              "    <h2>Ejemplo de Código con NLTK</h2>\n",
              "    <p>El siguiente código muestra cómo usar algunas de las funcionalidades más básicas de NLTK: tokenización, etiquetado gramatical (POS tagging), stemming, y lematización.</p>\n",
              "\n",
              "    <pre><code>import nltk\n",
              "# Descargar recursos necesarios\n",
              "nltk.download('punkt')\n",
              "nltk.download('averaged_perceptron_tagger')\n",
              "nltk.download('wordnet')\n",
              "\n",
              "# Importar las funcionalidades necesarias\n",
              "from nltk.tokenize import word_tokenize\n",
              "from nltk import pos_tag\n",
              "from nltk.stem import PorterStemmer\n",
              "from nltk.stem import WordNetLemmatizer\n",
              "\n",
              "# Frase para analizar\n",
              "text = \"Artificial intelligence is going to revolutionize the world of work.\"\n",
              "\n",
              "# Tokenización: dividir el texto en palabras\n",
              "tokens = word_tokenize(text)\n",
              "print(\"Tokens:\", tokens)\n",
              "\n",
              "# Etiquetado gramatical: obtener la categoría gramatical de cada palabra\n",
              "tagged_tokens = pos_tag(tokens)\n",
              "print(\"Etiquetas gramaticales:\", tagged_tokens)\n",
              "\n",
              "# Stemming: reducir las palabras a su raíz\n",
              "stemmer = PorterStemmer()\n",
              "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
              "print(\"Stemming:\", stemmed_tokens)\n",
              "\n",
              "# Lematización: reducir las palabras a su forma base\n",
              "lemmatizer = WordNetLemmatizer()\n",
              "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
              "print(\"Lematización:\", lemmatized_tokens)</code></pre>\n",
              "\n",
              "    <h2>Explicación del Código</h2>\n",
              "    <ul>\n",
              "        <li><strong>Tokenización</strong>: Convierte el texto en una lista de palabras o tokens.</li>\n",
              "        <li><strong>Etiquetado gramatical</strong>: Asigna una categoría gramatical a cada palabra.</li>\n",
              "        <li><strong>Stemming</strong>: Reduce las palabras a su raíz o forma básica.</li>\n",
              "        <li><strong>Lematización</strong>: Convierte las palabras a su forma base o diccionario.</li>\n",
              "    </ul>\n",
              "\n",
              "    <h2>Salida esperada</h2>\n",
              "    <pre><code>Tokens: ['Artificial', 'intelligence', 'is', 'going', 'to', 'revolutionize', 'the', 'world', 'of', 'work', '.']\n",
              "Etiquetas gramaticales: [('Artificial', 'JJ'), ('intelligence', 'NN'), ('is', 'VBZ'), ('going', 'VBG'), ('to', 'TO'), ('revolutionize', 'VB'), ('the', 'DT'), ('world', 'NN'), ('of', 'IN'), ('work', 'NN'), ('.', '.')]\n",
              "Stemming: ['artifici', 'intellig', 'is', 'go', 'to', 'revolution', 'the', 'world', 'of', 'work', '.']\n",
              "Lematización: ['Artificial', 'intelligence', 'is', 'going', 'to', 'revolutionize', 'the', 'world', 'of', 'work', '.']</code></pre>\n",
              "\n",
              "    <h2>Conclusión</h2>\n",
              "    <p>NLTK es una poderosa biblioteca para el procesamiento del lenguaje natural. Con este código, has aprendido cómo tokenizar, etiquetar gramaticalmente, aplicar stemming y lematización a un texto. NLTK ofrece una forma rápida y sencilla de abordar tareas complejas de procesamiento de texto.</p>\n",
              "</body>\n",
              "</html>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "# Contenido HTML que explica qué es spaCy y cómo usarlo en Python\n",
        "html_content = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"es\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Introducción a spaCy</title>\n",
        "    <link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap\" rel=\"stylesheet\">\n",
        "    <style>\n",
        "        body {\n",
        "            font-family: 'Roboto', Arial, sans-serif;\n",
        "            line-height: 1.6;\n",
        "            margin: 20px;\n",
        "            padding: 20px;\n",
        "            background-color: #f9f9f9;\n",
        "            color: #333;\n",
        "        }\n",
        "        h1 {\n",
        "            color: #2c3e50;\n",
        "            text-align: center;\n",
        "            font-size: 2.5em;\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "        h2 {\n",
        "            color: #2980b9;\n",
        "            font-size: 1.8em;\n",
        "            margin-top: 30px;\n",
        "        }\n",
        "        p {\n",
        "            font-size: 1.2em;\n",
        "            margin-bottom: 15px;\n",
        "        }\n",
        "        code {\n",
        "            background-color: #f0f0f0;\n",
        "            padding: 2px 5px;\n",
        "            border-radius: 3px;\n",
        "            font-family: 'Courier New', Courier, monospace;\n",
        "        }\n",
        "        pre {\n",
        "            background-color: #f0f0f0;\n",
        "            padding: 10px;\n",
        "            border-radius: 5px;\n",
        "            font-size: 1.1em;\n",
        "            overflow-x: auto;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>¿Qué es spaCy?</h1>\n",
        "    <p><strong>spaCy</strong> es una biblioteca poderosa y moderna para el procesamiento del lenguaje natural (NLP) en Python, optimizada para el uso en aplicaciones del mundo real. Su enfoque está en el rendimiento, y es fácil de usar en producción.</p>\n",
        "\n",
        "    <h2>Principales características de spaCy:</h2>\n",
        "    <ul>\n",
        "        <li><strong>Tokenización avanzada</strong>: Divide el texto en palabras o frases de manera eficiente.</li>\n",
        "        <li><strong>Etiquetado gramatical (POS tagging)</strong>: Asigna categorías gramaticales a las palabras (sustantivos, verbos, etc.).</li>\n",
        "        <li><strong>Lematización</strong>: Devuelve la forma base de una palabra (por ejemplo, infinitivos de verbos).</li>\n",
        "        <li><strong>Dependencia sintáctica</strong>: Analiza las relaciones gramaticales entre palabras.</li>\n",
        "        <li><strong>Reconocimiento de entidades nombradas (NER)</strong>: Detecta entidades como nombres de personas, fechas, lugares, etc.</li>\n",
        "    </ul>\n",
        "\n",
        "    <h2>Ejemplo práctico utilizando spaCy</h2>\n",
        "    <p>El siguiente código muestra cómo usar spaCy para realizar tokenización, etiquetado gramatical, lematización y obtener más detalles de los tokens en una oración.</p>\n",
        "\n",
        "    <pre><code>import spacy\n",
        "\n",
        "# Cargar el modelo en inglés (pequeño)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Frase para analizar\n",
        "text = \"Artificial intelligence is going to revolutionize the world of work.\"\n",
        "\n",
        "# Procesar el texto\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenización: Dividir el texto en tokens (palabras)\n",
        "tokens = [token.text for token in doc]\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Etiquetado gramatical (POS tagging): Obtener la categoría gramatical de cada palabra\n",
        "pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "print(\"Etiquetas gramaticales (POS):\", pos_tags)\n",
        "\n",
        "# Lematización: Reducir las palabras a su forma base\n",
        "lemmas = [(token.text, token.lemma_) for token in doc]\n",
        "print(\"Lematización:\", lemmas)\n",
        "\n",
        "# Información adicional de los tokens\n",
        "token_details = [(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop) for token in doc]\n",
        "print(\"\\\\nDetalles de cada token:\")\n",
        "for detail in token_details:\n",
        "    print(detail)</code></pre>\n",
        "\n",
        "    <h2>Explicación detallada del código</h2>\n",
        "    <ul>\n",
        "        <li><strong>Carga del modelo:</strong> spaCy utiliza un modelo preentrenado en inglés, que contiene las reglas gramaticales y semánticas necesarias.</li>\n",
        "        <li><strong>Tokenización:</strong> Divide la frase en palabras y signos de puntuación.</li>\n",
        "        <li><strong>Etiquetado gramatical (POS tagging):</strong> Asigna una categoría gramatical a cada palabra.</li>\n",
        "        <li><strong>Lematización:</strong> Devuelve la forma base o raíz de cada palabra.</li>\n",
        "        <li><strong>Detalles de los tokens:</strong> spaCy ofrece información adicional sobre cada palabra, como su función en la oración, si es una palabra alfabética o una palabra de parada (stop word).</li>\n",
        "    </ul>\n",
        "\n",
        "    <h2>Salida esperada:</h2>\n",
        "    <pre><code>Tokens: ['Artificial', 'intelligence', 'is', 'going', 'to', 'revolutionize', 'the', 'world', 'of', 'work', '.']\n",
        "Etiquetas gramaticales (POS): [('Artificial', 'ADJ'), ('intelligence', 'NOUN'), ('is', 'AUX'), ('going', 'VERB'), ('to', 'PART'), ('revolutionize', 'VERB'), ('the', 'DET'), ('world', 'NOUN'), ('of', 'ADP'), ('work', 'NOUN'), ('.', 'PUNCT')]\n",
        "Lematización: [('Artificial', 'artificial'), ('intelligence', 'intelligence'), ('is', 'be'), ('going', 'go'), ('to', 'to'), ('revolutionize', 'revolutionize'), ('the', 'the'), ('world', 'world'), ('of', 'of'), ('work', 'work'), ('.', '.')]\n",
        "Detalles de cada token: [('Artificial', 'artificial', 'ADJ', 'JJ', 'amod', 'Xxxxx', True, False), ...]</code></pre>\n",
        "\n",
        "    <h2>Conclusión</h2>\n",
        "    <p>spaCy ofrece una solución poderosa y eficiente para el procesamiento del lenguaje natural, permitiendo analizar, tokenizar, etiquetar y lematizar texto de manera rápida y precisa. Es ideal para aplicaciones de NLP en producción.</p>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Mostrar el contenido HTML en Google Colab\n",
        "display(HTML(html_content))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1IhGNZXSEY6g",
        "outputId": "9316904a-c892-4519-bac7-1be372d60ad0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<!DOCTYPE html>\n",
              "<html lang=\"es\">\n",
              "<head>\n",
              "    <meta charset=\"UTF-8\">\n",
              "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
              "    <title>Introducción a spaCy</title>\n",
              "    <link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap\" rel=\"stylesheet\">\n",
              "    <style>\n",
              "        body {\n",
              "            font-family: 'Roboto', Arial, sans-serif;\n",
              "            line-height: 1.6;\n",
              "            margin: 20px;\n",
              "            padding: 20px;\n",
              "            background-color: #f9f9f9;\n",
              "            color: #333;\n",
              "        }\n",
              "        h1 {\n",
              "            color: #2c3e50;\n",
              "            text-align: center;\n",
              "            font-size: 2.5em;\n",
              "            margin-bottom: 20px;\n",
              "        }\n",
              "        h2 {\n",
              "            color: #2980b9;\n",
              "            font-size: 1.8em;\n",
              "            margin-top: 30px;\n",
              "        }\n",
              "        p {\n",
              "            font-size: 1.2em;\n",
              "            margin-bottom: 15px;\n",
              "        }\n",
              "        code {\n",
              "            background-color: #f0f0f0;\n",
              "            padding: 2px 5px;\n",
              "            border-radius: 3px;\n",
              "            font-family: 'Courier New', Courier, monospace;\n",
              "        }\n",
              "        pre {\n",
              "            background-color: #f0f0f0;\n",
              "            padding: 10px;\n",
              "            border-radius: 5px;\n",
              "            font-size: 1.1em;\n",
              "            overflow-x: auto;\n",
              "        }\n",
              "    </style>\n",
              "</head>\n",
              "<body>\n",
              "    <h1>¿Qué es spaCy?</h1>\n",
              "    <p><strong>spaCy</strong> es una biblioteca poderosa y moderna para el procesamiento del lenguaje natural (NLP) en Python, optimizada para el uso en aplicaciones del mundo real. Su enfoque está en el rendimiento, y es fácil de usar en producción.</p>\n",
              "    \n",
              "    <h2>Principales características de spaCy:</h2>\n",
              "    <ul>\n",
              "        <li><strong>Tokenización avanzada</strong>: Divide el texto en palabras o frases de manera eficiente.</li>\n",
              "        <li><strong>Etiquetado gramatical (POS tagging)</strong>: Asigna categorías gramaticales a las palabras (sustantivos, verbos, etc.).</li>\n",
              "        <li><strong>Lematización</strong>: Devuelve la forma base de una palabra (por ejemplo, infinitivos de verbos).</li>\n",
              "        <li><strong>Dependencia sintáctica</strong>: Analiza las relaciones gramaticales entre palabras.</li>\n",
              "        <li><strong>Reconocimiento de entidades nombradas (NER)</strong>: Detecta entidades como nombres de personas, fechas, lugares, etc.</li>\n",
              "    </ul>\n",
              "\n",
              "    <h2>Ejemplo práctico utilizando spaCy</h2>\n",
              "    <p>El siguiente código muestra cómo usar spaCy para realizar tokenización, etiquetado gramatical, lematización y obtener más detalles de los tokens en una oración.</p>\n",
              "\n",
              "    <pre><code>import spacy\n",
              "\n",
              "# Cargar el modelo en inglés (pequeño)\n",
              "nlp = spacy.load(\"en_core_web_sm\")\n",
              "\n",
              "# Frase para analizar\n",
              "text = \"Artificial intelligence is going to revolutionize the world of work.\"\n",
              "\n",
              "# Procesar el texto\n",
              "doc = nlp(text)\n",
              "\n",
              "# Tokenización: Dividir el texto en tokens (palabras)\n",
              "tokens = [token.text for token in doc]\n",
              "print(\"Tokens:\", tokens)\n",
              "\n",
              "# Etiquetado gramatical (POS tagging): Obtener la categoría gramatical de cada palabra\n",
              "pos_tags = [(token.text, token.pos_) for token in doc]\n",
              "print(\"Etiquetas gramaticales (POS):\", pos_tags)\n",
              "\n",
              "# Lematización: Reducir las palabras a su forma base\n",
              "lemmas = [(token.text, token.lemma_) for token in doc]\n",
              "print(\"Lematización:\", lemmas)\n",
              "\n",
              "# Información adicional de los tokens\n",
              "token_details = [(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop) for token in doc]\n",
              "print(\"\\nDetalles de cada token:\")\n",
              "for detail in token_details:\n",
              "    print(detail)</code></pre>\n",
              "\n",
              "    <h2>Explicación detallada del código</h2>\n",
              "    <ul>\n",
              "        <li><strong>Carga del modelo:</strong> spaCy utiliza un modelo preentrenado en inglés, que contiene las reglas gramaticales y semánticas necesarias.</li>\n",
              "        <li><strong>Tokenización:</strong> Divide la frase en palabras y signos de puntuación.</li>\n",
              "        <li><strong>Etiquetado gramatical (POS tagging):</strong> Asigna una categoría gramatical a cada palabra.</li>\n",
              "        <li><strong>Lematización:</strong> Devuelve la forma base o raíz de cada palabra.</li>\n",
              "        <li><strong>Detalles de los tokens:</strong> spaCy ofrece información adicional sobre cada palabra, como su función en la oración, si es una palabra alfabética o una palabra de parada (stop word).</li>\n",
              "    </ul>\n",
              "\n",
              "    <h2>Salida esperada:</h2>\n",
              "    <pre><code>Tokens: ['Artificial', 'intelligence', 'is', 'going', 'to', 'revolutionize', 'the', 'world', 'of', 'work', '.']\n",
              "Etiquetas gramaticales (POS): [('Artificial', 'ADJ'), ('intelligence', 'NOUN'), ('is', 'AUX'), ('going', 'VERB'), ('to', 'PART'), ('revolutionize', 'VERB'), ('the', 'DET'), ('world', 'NOUN'), ('of', 'ADP'), ('work', 'NOUN'), ('.', 'PUNCT')]\n",
              "Lematización: [('Artificial', 'artificial'), ('intelligence', 'intelligence'), ('is', 'be'), ('going', 'go'), ('to', 'to'), ('revolutionize', 'revolutionize'), ('the', 'the'), ('world', 'world'), ('of', 'of'), ('work', 'work'), ('.', '.')]\n",
              "Detalles de cada token: [('Artificial', 'artificial', 'ADJ', 'JJ', 'amod', 'Xxxxx', True, False), ...]</code></pre>\n",
              "\n",
              "    <h2>Conclusión</h2>\n",
              "    <p>spaCy ofrece una solución poderosa y eficiente para el procesamiento del lenguaje natural, permitiendo analizar, tokenizar, etiquetar y lematizar texto de manera rápida y precisa. Es ideal para aplicaciones de NLP en producción.</p>\n",
              "</body>\n",
              "</html>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uJVkF7D-_3N",
        "outputId": "23231b71-1a34-4af0-a210-ed47d8479678"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Cargar el modelo en inglés (pequeño)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Frase para analizar\n",
        "text = \"Artificial intelligence is going to revolutionize the world of work.\"\n",
        "\n",
        "# Procesar el texto\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenización: Dividir el texto en tokens (palabras)\n",
        "tokens = [token.text for token in doc]\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Etiquetado gramatical (POS tagging): Obtener la categoría gramatical de cada palabra\n",
        "pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "print(\"Etiquetas gramaticales (POS):\", pos_tags)\n",
        "\n",
        "# Lematización: Reducir las palabras a su forma base\n",
        "lemmas = [(token.text, token.lemma_) for token in doc]\n",
        "print(\"Lematización:\", lemmas)\n",
        "\n",
        "# Información adicional de los tokens\n",
        "token_details = [(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop) for token in doc]\n",
        "print(\"\\nDetalles de cada token:\")\n",
        "for detail in token_details:\n",
        "    print(detail)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEpHlnQkDOWI",
        "outputId": "e4e053ab-1a4c-4100-fba2-80c1eeee80ec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Artificial', 'intelligence', 'is', 'going', 'to', 'revolutionize', 'the', 'world', 'of', 'work', '.']\n",
            "Etiquetas gramaticales (POS): [('Artificial', 'ADJ'), ('intelligence', 'NOUN'), ('is', 'AUX'), ('going', 'VERB'), ('to', 'PART'), ('revolutionize', 'VERB'), ('the', 'DET'), ('world', 'NOUN'), ('of', 'ADP'), ('work', 'NOUN'), ('.', 'PUNCT')]\n",
            "Lematización: [('Artificial', 'artificial'), ('intelligence', 'intelligence'), ('is', 'be'), ('going', 'go'), ('to', 'to'), ('revolutionize', 'revolutionize'), ('the', 'the'), ('world', 'world'), ('of', 'of'), ('work', 'work'), ('.', '.')]\n",
            "\n",
            "Detalles de cada token:\n",
            "('Artificial', 'artificial', 'ADJ', 'JJ', 'amod', 'Xxxxx', True, False)\n",
            "('intelligence', 'intelligence', 'NOUN', 'NN', 'nsubj', 'xxxx', True, False)\n",
            "('is', 'be', 'AUX', 'VBZ', 'aux', 'xx', True, True)\n",
            "('going', 'go', 'VERB', 'VBG', 'ROOT', 'xxxx', True, False)\n",
            "('to', 'to', 'PART', 'TO', 'aux', 'xx', True, True)\n",
            "('revolutionize', 'revolutionize', 'VERB', 'VB', 'xcomp', 'xxxx', True, False)\n",
            "('the', 'the', 'DET', 'DT', 'det', 'xxx', True, True)\n",
            "('world', 'world', 'NOUN', 'NN', 'dobj', 'xxxx', True, False)\n",
            "('of', 'of', 'ADP', 'IN', 'prep', 'xx', True, True)\n",
            "('work', 'work', 'NOUN', 'NN', 'pobj', 'xxxx', True, False)\n",
            "('.', '.', 'PUNCT', '.', 'punct', '.', False, False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zBWv3ryJDOyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Cargar el modelo en español\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Frase a tokenizar\n",
        "text = \"La inteligencia artificial está transformando el mundo rápidamente.\"\n",
        "\n",
        "# Procesar el texto\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extraer los tokens\n",
        "tokens = [token.text for token in doc]\n",
        "\n",
        "# Imprimir los tokens\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukKf0O2T_DJj",
        "outputId": "582b55b1-7fee-436e-815d-7c9e94410695"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['La', 'inteligencia', 'artificial', 'está', 'transformando', 'el', 'mundo', 'rápidamente', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe14SnuG_TXQ",
        "outputId": "0390f55e-f668-4c65-c871-24e5aae7b283"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    from IPython.display import display, HTML\n",
        "\n",
        "# Contenido HTML que explica cómo funciona el Tokenizador de Hugging Face con BERT\n",
        "html_content = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"es\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Tokenización con Hugging Face y BERT</title>\n",
        "    <link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap\" rel=\"stylesheet\">\n",
        "    <style>\n",
        "        body {\n",
        "            font-family: 'Roboto', Arial, sans-serif;\n",
        "            line-height: 1.6;\n",
        "            margin: 20px;\n",
        "            padding: 20px;\n",
        "            background-color: #f9f9f9;\n",
        "            color: #333;\n",
        "        }\n",
        "        h1 {\n",
        "            color: #2c3e50;\n",
        "            text-align: center;\n",
        "            font-size: 2.5em;\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "        h2 {\n",
        "            color: #2980b9;\n",
        "            font-size: 1.8em;\n",
        "            margin-top: 30px;\n",
        "        }\n",
        "        p {\n",
        "            font-size: 1.2em;\n",
        "            margin-bottom: 15px;\n",
        "        }\n",
        "        code {\n",
        "            background-color: #f0f0f0;\n",
        "            padding: 2px 5px;\n",
        "            border-radius: 3px;\n",
        "            font-family: 'Courier New', Courier, monospace;\n",
        "        }\n",
        "        pre {\n",
        "            background-color: #f0f0f0;\n",
        "            padding: 10px;\n",
        "            border-radius: 5px;\n",
        "            font-size: 1.1em;\n",
        "            overflow-x: auto;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Tokenización con Hugging Face y BERT</h1>\n",
        "    <p>En este tutorial, aprenderás cómo utilizar el <strong>Tokenizador de Hugging Face</strong> con el modelo <strong>BERT</strong> para realizar tareas de procesamiento del lenguaje natural (NLP), como la tokenización y la decodificación.</p>\n",
        "\n",
        "    <h2>¿Qué es un Tokenizador?</h2>\n",
        "    <p>Un <strong>tokenizador</strong> divide el texto en palabras, subpalabras o caracteres. Modelos como <strong>BERT</strong> utilizan un tokenizador de tipo <em>WordPiece</em>, que descompone palabras desconocidas en subpalabras más pequeñas.</p>\n",
        "\n",
        "    <h2>Código de Ejemplo</h2>\n",
        "    <p>El siguiente código muestra cómo tokenizar una frase en inglés, obtener los identificadores de los tokens (IDs), agregar los tokens especiales [CLS] y [SEP], y finalmente decodificar los tokens nuevamente a texto.</p>\n",
        "\n",
        "    <pre><code>from transformers import AutoTokenizer\n",
        "\n",
        "# Cargar un tokenizador preentrenado (BERT en inglés)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Frase para tokenizar\n",
        "text = \"Artificial intelligence is going to revolutionize the world of work.\"\n",
        "\n",
        "# Tokenización\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Convertir tokens a IDs\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"IDs de los tokens:\", token_ids)\n",
        "\n",
        "# Agregar tokens especiales [CLS] y [SEP]\n",
        "tokens_with_special = tokenizer.encode(text)\n",
        "print(\"Tokens con [CLS] y [SEP]:\", tokens_with_special)\n",
        "\n",
        "# Decodificación de los IDs a texto\n",
        "decoded_text = tokenizer.decode(tokens_with_special)\n",
        "print(\"Texto decodificado:\", decoded_text)</code></pre>\n",
        "\n",
        "    <h2>Explicación del Código</h2>\n",
        "    <ul>\n",
        "        <li><strong>Tokenización:</strong> Convierte el texto en tokens utilizando el tokenizador de BERT.</li>\n",
        "        <li><strong>IDs de los tokens:</strong> Convierte los tokens en sus correspondientes identificadores numéricos en el vocabulario de BERT.</li>\n",
        "        <li><strong>Tokens especiales:</strong> Agrega [CLS] (inicio) y [SEP] (fin) a la secuencia tokenizada.</li>\n",
        "        <li><strong>Decodificación:</strong> Convierte los IDs de vuelta a texto.</li>\n",
        "    </ul>\n",
        "\n",
        "    <h2>Ejemplo de Salida</h2>\n",
        "    <pre><code>Tokens: ['artificial', 'intelligence', 'is', 'going', 'to', 're', '##vol', '##ution', '##ize', 'the', 'world', 'of', 'work', '.']\n",
        "IDs de los tokens: [21328, 17515, 2003, 2183, 2000, 2128, 14893, 21107, 1045, 1996, 2088, 1997, 2147, 1012]\n",
        "Tokens con [CLS] y [SEP]: [101, 21328, 17515, 2003, 2183, 2000, 2128, 14893, 21107, 1045, 1996, 2088, 1997, 2147, 1012, 102]\n",
        "Texto decodificado: artificial intelligence is going to revolutionize the world of work.</code></pre>\n",
        "\n",
        "    <h2>Conclusión</h2>\n",
        "    <p>Este ejemplo muestra cómo puedes utilizar el tokenizador de Hugging Face para dividir texto en tokens, obtener los IDs de los tokens, agregar tokens especiales y decodificar los IDs a texto. Es un proceso fundamental en el trabajo con modelos como <strong>BERT</strong> en tareas de NLP.</p>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Mostrar el contenido HTML en Google Colab\n",
        "display(HTML(html_content))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "01ge_IA5HXna",
        "outputId": "a088f246-3362-4523-a0b9-9695265cae29"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<!DOCTYPE html>\n",
              "<html lang=\"es\">\n",
              "<head>\n",
              "<meta charset=\"UTF-8\">\n",
              "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
              "<title>Tokenización con Hugging Face y BERT</title>\n",
              "<link href=\"https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap\" rel=\"stylesheet\">\n",
              "<style>\n",
              "    body {\n",
              "        font-family: 'Roboto', Arial, sans-serif;\n",
              "        line-height: 1.6;\n",
              "        margin: 20px;\n",
              "        padding: 20px;\n",
              "        background-color: #f9f9f9;\n",
              "        color: #333;\n",
              "    }\n",
              "    h1 {\n",
              "        color: #2c3e50;\n",
              "        text-align: center;\n",
              "        font-size: 2.5em;\n",
              "        margin-bottom: 20px;\n",
              "    }\n",
              "    h2 {\n",
              "        color: #2980b9;\n",
              "        font-size: 1.8em;\n",
              "        margin-top: 30px;\n",
              "    }\n",
              "    p {\n",
              "        font-size: 1.2em;\n",
              "        margin-bottom: 15px;\n",
              "    }\n",
              "    code {\n",
              "        background-color: #f0f0f0;\n",
              "        padding: 2px 5px;\n",
              "        border-radius: 3px;\n",
              "        font-family: 'Courier New', Courier, monospace;\n",
              "    }\n",
              "    pre {\n",
              "        background-color: #f0f0f0;\n",
              "        padding: 10px;\n",
              "        border-radius: 5px;\n",
              "        font-size: 1.1em;\n",
              "        overflow-x: auto;\n",
              "    }\n",
              "</style>\n",
              "</head>\n",
              "<body>\n",
              "<h1>Tokenización con Hugging Face y BERT</h1>\n",
              "<p>En este tutorial, aprenderás cómo utilizar el <strong>Tokenizador de Hugging Face</strong> con el modelo <strong>BERT</strong> para realizar tareas de procesamiento del lenguaje natural (NLP), como la tokenización y la decodificación.</p>\n",
              "\n",
              "<h2>¿Qué es un Tokenizador?</h2>\n",
              "<p>Un <strong>tokenizador</strong> divide el texto en palabras, subpalabras o caracteres. Modelos como <strong>BERT</strong> utilizan un tokenizador de tipo <em>WordPiece</em>, que descompone palabras desconocidas en subpalabras más pequeñas.</p>\n",
              "\n",
              "<h2>Código de Ejemplo</h2>\n",
              "<p>El siguiente código muestra cómo tokenizar una frase en inglés, obtener los identificadores de los tokens (IDs), agregar los tokens especiales [CLS] y [SEP], y finalmente decodificar los tokens nuevamente a texto.</p>\n",
              "\n",
              "<pre><code>from transformers import AutoTokenizer\n",
              "\n",
              "# Cargar un tokenizador preentrenado (BERT en inglés)\n",
              "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
              "\n",
              "# Frase para tokenizar\n",
              "text = \"Artificial intelligence is going to revolutionize the world of work.\"\n",
              "\n",
              "# Tokenización\n",
              "tokens = tokenizer.tokenize(text)\n",
              "print(\"Tokens:\", tokens)\n",
              "\n",
              "# Convertir tokens a IDs\n",
              "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
              "print(\"IDs de los tokens:\", token_ids)\n",
              "\n",
              "# Agregar tokens especiales [CLS] y [SEP]\n",
              "tokens_with_special = tokenizer.encode(text)\n",
              "print(\"Tokens con [CLS] y [SEP]:\", tokens_with_special)\n",
              "\n",
              "# Decodificación de los IDs a texto\n",
              "decoded_text = tokenizer.decode(tokens_with_special)\n",
              "print(\"Texto decodificado:\", decoded_text)</code></pre>\n",
              "\n",
              "<h2>Explicación del Código</h2>\n",
              "<ul>\n",
              "    <li><strong>Tokenización:</strong> Convierte el texto en tokens utilizando el tokenizador de BERT.</li>\n",
              "    <li><strong>IDs de los tokens:</strong> Convierte los tokens en sus correspondientes identificadores numéricos en el vocabulario de BERT.</li>\n",
              "    <li><strong>Tokens especiales:</strong> Agrega [CLS] (inicio) y [SEP] (fin) a la secuencia tokenizada.</li>\n",
              "    <li><strong>Decodificación:</strong> Convierte los IDs de vuelta a texto.</li>\n",
              "</ul>\n",
              "\n",
              "<h2>Ejemplo de Salida</h2>\n",
              "<pre><code>Tokens: ['artificial', 'intelligence', 'is', 'going', 'to', 're', '##vol', '##ution', '##ize', 'the', 'world', 'of', 'work', '.']\n",
              "IDs de los tokens: [21328, 17515, 2003, 2183, 2000, 2128, 14893, 21107, 1045, 1996, 2088, 1997, 2147, 1012]\n",
              "Tokens con [CLS] y [SEP]: [101, 21328, 17515, 2003, 2183, 2000, 2128, 14893, 21107, 1045, 1996, 2088, 1997, 2147, 1012, 102]\n",
              "Texto decodificado: artificial intelligence is going to revolutionize the world of work.</code></pre>\n",
              "\n",
              "<h2>Conclusión</h2>\n",
              "<p>Este ejemplo muestra cómo puedes utilizar el tokenizador de Hugging Face para dividir texto en tokens, obtener los IDs de los tokens, agregar tokens especiales y decodificar los IDs a texto. Es un proceso fundamental en el trabajo con modelos como <strong>BERT</strong> en tareas de NLP.</p>\n",
              "</body>\n",
              "</html>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WET-74U7_pIm",
        "outputId": "8d0372ed-cfcb-4861-9c8b-17957fb928cb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Cargar un tokenizador preentrenado (por ejemplo, BERT en inglés)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Frase en inglés para tokenizar\n",
        "text = \"Artificial intelligence is going to revolutionize the world of work.\"\n",
        "\n",
        "# Tokenizar la frase\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Obtener los IDs de los tokens (números que representan cada token en el vocabulario de BERT)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"IDs de los tokens:\", token_ids)\n",
        "\n",
        "# Agregar los tokens especiales [CLS] (inicio) y [SEP] (separador/final) como BERT espera\n",
        "tokens_with_special = tokenizer.encode(text)\n",
        "print(\"Tokens con [CLS] y [SEP]:\", tokens_with_special)\n",
        "\n",
        "# Decodificar los IDs de tokens a texto (reconstruir el texto a partir de los tokens)\n",
        "decoded_text = tokenizer.decode(tokens_with_special)\n",
        "print(\"Texto decodificado:\", decoded_text)\n",
        "\n",
        "# Información adicional de los tokens\n",
        "token_info = [{\"token\": token, \"id\": token_id} for token, token_id in zip(tokens, token_ids)]\n",
        "print(\"\\nInformación detallada de los tokens:\")\n",
        "for info in token_info:\n",
        "    print(info)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtewC75N_s1F",
        "outputId": "deb83457-a7ae-4fd7-8467-2c354f27820f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['artificial', 'intelligence', 'is', 'going', 'to', 'revolution', '##ize', 'the', 'world', 'of', 'work', '.']\n",
            "IDs de los tokens: [7976, 4454, 2003, 2183, 2000, 4329, 4697, 1996, 2088, 1997, 2147, 1012]\n",
            "Tokens con [CLS] y [SEP]: [101, 7976, 4454, 2003, 2183, 2000, 4329, 4697, 1996, 2088, 1997, 2147, 1012, 102]\n",
            "Texto decodificado: [CLS] artificial intelligence is going to revolutionize the world of work. [SEP]\n",
            "\n",
            "Información detallada de los tokens:\n",
            "{'token': 'artificial', 'id': 7976}\n",
            "{'token': 'intelligence', 'id': 4454}\n",
            "{'token': 'is', 'id': 2003}\n",
            "{'token': 'going', 'id': 2183}\n",
            "{'token': 'to', 'id': 2000}\n",
            "{'token': 'revolution', 'id': 4329}\n",
            "{'token': '##ize', 'id': 4697}\n",
            "{'token': 'the', 'id': 1996}\n",
            "{'token': 'world', 'id': 2088}\n",
            "{'token': 'of', 'id': 1997}\n",
            "{'token': 'work', 'id': 2147}\n",
            "{'token': '.', 'id': 1012}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}